{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Bug Prediction with Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this project is to apply Deep Learning NLP based techniques to predict the bugs, features, and questions based on GitHub titles and the text body.\n",
    "\n",
    "We need to predict\n",
    "\n",
    "    ● Bug - 0\n",
    "    ● Feature - 1\n",
    "    ● Question - 2\n",
    "given Github titles and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install sagemaker, kaggle and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.4.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.16.37)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.14.12->sagemaker==1.72.0) (1.25.11)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.37)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: kaggle in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.5.10)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (4.42.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2.25.0)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (1.25.11)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2020.11.8)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2020.11.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->kaggle) (3.0.4)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (1.25.11)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.2.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.19.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.25.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.11.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0\n",
    "!pip install kaggle\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries which will be required in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tqdm.autonotebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import sagemaker related libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create directory named kaggle which will have json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir .kaggle\n",
    "!touch .kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create kaggle.json file which will have api key to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = {\"username\":\"codewarrior0\",\"key\":\"c01b65090e3a59a881fd19675d7a54e1\"}\n",
    "\n",
    "with open('/home/ec2-user/SageMaker/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 /home/ec2-user/SageMaker/.kaggle/kaggle.json\n",
    "%mv .kaggle /home/ec2-user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create data directory named kaggle which will have all the data downloaded from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading github-bugs-prediction.zip to /home/ec2-user/SageMaker\n",
      " 88%|██████████████████████████████████▌    | 87.0M/98.3M [00:00<00:00, 104MB/s]\n",
      "100%|███████████████████████████████████████| 98.3M/98.3M [00:00<00:00, 109MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d anmolkumar/github-bugs-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  github-bugs-prediction.zip\n",
      "  inflating: embold_test.json        \n",
      "  inflating: embold_train.json       \n",
      "  inflating: embold_train_extra.json  \n",
      "  inflating: sample submission.csv   \n"
     ]
    }
   ],
   "source": [
    "!unzip github-bugs-prediction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move train and test data to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘github-bugs-prediction.zip’: No such file or directory\n",
      "rm: cannot remove ‘sample submission.csv’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%mv embold_train.json kaggle/train.json\n",
    "%mv embold_test.json kaggle/test.json\n",
    "%mv embold_train_extra.json kaggle/train_extra.json\n",
    "%rm github-bugs-prediction.zip\n",
    "%rm sample\\ submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data in a dataframe and see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \n",
       "0        a y-zoom on the piano roll would be useful.      1  \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0  \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1  \n",
       "3  i think we should stop logging requests to:\\r ...      1  \n",
       "4  expected behavior\\r alarm actions pid on and p...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= pd.read_json(\"./kaggle/train.json\").reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data in dataframe and see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>config question  path-specific environment var...</td>\n",
       "      <td>issue description or question\\r \\r hey @artemg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash indien vol</td>\n",
       "      <td>de simulator crasht als hij vol zit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unable to mine rocks</td>\n",
       "      <td>sarkasmo starting today, when i hit enter  act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not all whitelists are processed</td>\n",
       "      <td>create following rules... order of creation is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>add ctx menu for idafree 70 and idafree 5</td>\n",
       "      <td>associated with .dll, .dll_, .exe, .exe_, .sc,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  config question  path-specific environment var...   \n",
       "1                                   crash indien vol   \n",
       "2                               unable to mine rocks   \n",
       "3                   not all whitelists are processed   \n",
       "4          add ctx menu for idafree 70 and idafree 5   \n",
       "\n",
       "                                                body  \n",
       "0  issue description or question\\r \\r hey @artemg...  \n",
       "1                de simulator crasht als hij vol zit  \n",
       "2  sarkasmo starting today, when i hit enter  act...  \n",
       "3  create following rules... order of creation is...  \n",
       "4  associated with .dll, .dll_, .exe, .exe_, .sc,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df= pd.read_json(\"./kaggle/test.json\").reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load extra training data in dataframe and see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use a 8bit typeface</td>\n",
       "      <td>since this is meant to emulate some old arcade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>implement wireless m-bus binding</td>\n",
       "      <td>_from  chris.pa...@googlemail.com  https://cod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add multilang support for timeago.js</td>\n",
       "      <td>currently it is only  en . \\r required to add ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scaleway - seg-fault on shutdown</td>\n",
       "      <td>tbr  irc  creates a new scaleway instance with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sistema de pintura: no se guardar los nuevos p...</td>\n",
       "      <td>este sp ya estaba asignado a un carro y se enc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                use a 8bit typeface   \n",
       "1                   implement wireless m-bus binding   \n",
       "2               add multilang support for timeago.js   \n",
       "3                   scaleway - seg-fault on shutdown   \n",
       "4  sistema de pintura: no se guardar los nuevos p...   \n",
       "\n",
       "                                                body  label  \n",
       "0  since this is meant to emulate some old arcade...      1  \n",
       "1  _from  chris.pa...@googlemail.com  https://cod...      1  \n",
       "2  currently it is only  en . \\r required to add ...      1  \n",
       "3  tbr  irc  creates a new scaleway instance with...      0  \n",
       "4  este sp ya estaba asignado a un carro y se enc...      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_extra_df= pd.read_json(\"./kaggle/train_extra.json\").reset_index(drop=True)\n",
    "train_extra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in train.json and train_extra.json is similar. Combine the data in both files as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 150000\n",
      "300000 300000\n",
      "30000 30000\n"
     ]
    }
   ],
   "source": [
    "def dataset_length_check(data_frame):\n",
    "    print(len(data_frame),data_frame.index.shape[-1])\n",
    "                 \n",
    "dataset_length_check(train_df)\n",
    "dataset_length_check(train_extra_df)\n",
    "dataset_length_check(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>yzoom piano roll a yzoom on the piano roll wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection  screenshot from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi  great job so far saenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on  pid off alarm actions for  expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1  buggy behavior in selection  screenshot from  ...  \n",
       "2  auto update feature hi  great job so far saenz...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on  pid off alarm actions for  expe...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame = train_df[:50000]\n",
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the combiled length of training data. It should be 450000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   50000 non-null  object\n",
      " 1   body    50000 non-null  object\n",
      " 2   label   50000 non-null  int64 \n",
      " 3   text    50000 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "training_data_frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets categorize the data based on labels and name them as df_bug, df_feature and df_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Counts of label column: \n",
      " 1    23044\n",
      "0    22310\n",
      "2     4646\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fee72cda0b8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Total Counts of label column: \\n'.format(),training_data_frame['label'].value_counts())\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.countplot(x='label', data=training_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bug = training_data_frame[training_data_frame['label']==0]\n",
    "df_feature = training_data_frame[training_data_frame['label']==1]\n",
    "df_question = training_data_frame[training_data_frame['label']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22310\n",
       "1    23044\n",
       "2     4646\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = training_data_frame.label.value_counts().sort_index()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine the text in title and body and create new column with name text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x):\n",
    "    return x['title'] + \" \" + x['body']   \n",
    "training_data_frame['text']= training_data_frame.apply(lambda row : combine(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints with label as Bug : 22310\n",
      "Number of datapoints with label as Feature : 23044\n",
      "Number of datapoints with label as Question : 4646\n"
     ]
    }
   ],
   "source": [
    "print('Number of datapoints with label as Bug :',label_counts[0])\n",
    "print('Number of datapoints with label as Feature :',label_counts[1])\n",
    "print('Number of datapoints with label as Question :',label_counts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>y-zoom piano roll a y-zoom on the piano roll w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection ! screenshot from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi,\\r \\r great job so far,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  y-zoom piano roll a y-zoom on the piano roll w...  \n",
       "1  buggy behavior in selection ! screenshot from ...  \n",
       "2  auto update feature hi,\\r \\r great job so far,...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on / pid off alarm actions for ardu...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing/Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we make text lowercase, remove text in square brackets, remove links, remove punctuation and remove words containing numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords & Punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(git_text):\n",
    "    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n",
    "    # remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n",
    "    # convert them back to sentences and split into words\n",
    "    # remove_punctuation = \"\".join(remove_punctuation).split()\n",
    "    filtered_git_text = [word.lower() for word in git_text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_git_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "def visulize_dataset(data_frame, category):\n",
    "    \n",
    "    # Let's apply the above two functions 'clean_text' and 'remove_stopwords' to the whole dataset\n",
    "\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\n",
    "    data_frame[\"text\"] = data_frame[\"text\"].apply(remove_stopwords)\n",
    "    \n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for i, j in data_frame.iterrows():\n",
    "        for word in j['text']:\n",
    "            word_list.append(word)\n",
    "        \n",
    "    count_dict = Counter(word_list)\n",
    "    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n",
    "    fig = px.histogram(most_common_words_df,\n",
    "                       x='word', \n",
    "                       y='count',\n",
    "                       title='Most common terms used while refering to a GitHub {}'.format(category),\n",
    "                       color_discrete_sequence=['#843B62'] )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing the text field after Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>yzoom piano roll a yzoom on the piano roll wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection  screenshot from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi  great job so far saenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on  pid off alarm actions for  expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1  buggy behavior in selection  screenshot from  ...  \n",
       "2  auto update feature hi  great job so far saenz...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on  pid off alarm actions for  expe...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "label 0: Bug\n",
    "label 1: Feature\n",
    "label 2: Question\n",
    "\"\"\"\n",
    "training_data_frame['text'] = training_data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "training_data_frame['text'] = training_data_frame['text'].apply(lambda x: clean_text(x))\n",
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_frame['text'] = training_data_frame[\"text\"].apply(remove_stopwords)\n",
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                    title  \\\n",
       "0                                      y-zoom piano roll   \n",
       "1                            buggy behavior in selection   \n",
       "2                                    auto update feature   \n",
       "3                     filter out noisy endpoints in logs   \n",
       "4      enable pid on / pid off alarm actions for ardu...   \n",
       "...                                                  ...   \n",
       "49995  weapon power meter increase upon shield loss, ...   \n",
       "49996  kas 1.0: rts-1 doesn't see all the resources o...   \n",
       "49997                               add a picking system   \n",
       "49998                               upgrade to ionic 3.0   \n",
       "49999                         implement tabbed interface   \n",
       "\n",
       "                                                    body  label  \\\n",
       "0            a y-zoom on the piano roll would be useful.      1   \n",
       "1      ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2      hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3      i think we should stop logging requests to:\\r ...      1   \n",
       "4      expected behavior\\r alarm actions pid on and p...      0   \n",
       "...                                                  ...    ...   \n",
       "49995  description\\r \\r when your ship is damaged  fo...      1   \n",
       "49996  it seems the \\ direction\\  of the rts-1 affect...      0   \n",
       "49997  i don't saw the picking system/engine into you...      1   \n",
       "49998  ionic version 3 has been released based on ang...      1   \n",
       "49999  because all interfaces must be tabbed.\\r \\r fo...      1   \n",
       "\n",
       "                                                    text  \n",
       "0      yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1      buggy behavior in selection  screenshot from  ...  \n",
       "2      auto update feature hi  great job so far saenz...  \n",
       "3      filter out noisy endpoints in logs i think we ...  \n",
       "4      enable pid on  pid off alarm actions for  expe...  \n",
       "...                                                  ...  \n",
       "49995  weapon power meter increase upon shield loss e...  \n",
       "49996  kas   doesnt see all the resources on the vess...  \n",
       "49997  add a picking system i dont saw the picking sy...  \n",
       "49998  upgrade to ionic  ionic version  has been rele...  \n",
       "49999  implement tabbed interface because all interfa...  \n",
       "\n",
       "[50000 rows x 4 columns]>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a directory name data which will hold the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the text and label column from our data for training. Extract text and label column from data_frame and write it in data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation = train_test_split(training_data_frame[['label', 'text']])\n",
    "train.to_csv(\"./data/train.csv\", index = False)\n",
    "validation.to_csv(\"./data/validation.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat same steps for testing data. Clean and preprocess it and write the data in test.csv file in data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining title and body and creating new column with name text\n",
    "test_df['text'] = test_df.apply(lambda row : combine(row),axis=1)\n",
    "# Clean the text column by removing punctuation, stopwords etc\n",
    "test_df['text'] = test_df['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n",
    "#test_df['text'] = test_df['text'].apply(lambda x: remove_stopwords(x))\n",
    "test_df.head()\n",
    "# Extract text and label column\n",
    "test = test_df[['text']]\n",
    "test.to_csv(\"./data/test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee9deca9d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sagemaker session, s3 bucket to upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'mlcapstone/bug_prediction'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = sagemaker_session.upload_data(\"./data/train.csv\", bucket=bucket, key_prefix=prefix)\n",
    "input_validation = sagemaker_session.upload_data(\"./data/validation.csv\", bucket=bucket, key_prefix=prefix)\n",
    "input_test = sagemaker_session.upload_data(\"./data/test.csv\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-31 17:03:29 Starting - Starting the training job...\n",
      "2021-01-31 17:03:32 Starting - Launching requested ML instances.........\n",
      "2021-01-31 17:05:03 Starting - Preparing the instances for training......\n",
      "2021-01-31 17:06:28 Downloading - Downloading input data......\n",
      "2021-01-31 17:07:12 Training - Downloading the training image............\n",
      "2021-01-31 17:09:29 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:30,565 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:30,596 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:33,617 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:33,944 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:33,945 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:33,945 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:33,945 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmppl9_u_04/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.8.2)\u001b[0m\n",
      "\u001b[34mCollecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 3)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.6/site-packages (from beautifulsoup4->-r requirements.txt (line 4)) (1.9.5)\u001b[0m\n",
      "\u001b[34mCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from html5lib->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 11)) (0.7)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 11)) (20.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 11)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 11)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 11)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: more-itertools in /opt/conda/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 11)) (8.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, sacremoses, default-user-module-name\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=c6396ea7c1b25e9b73ab82bb592b47ab4e808d265fd3e585a6d304b8ab959d02\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=faed40ad7b98639429fa21edfedc825bee5473e9b745b692147d27f225ff761c\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=25417 sha256=079e70c2a5ab6828c95920b256ee9c7e2295d1d18855de29f23013cd1d178688\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zn8ry2xp/wheels/7e/75/d1/81e48b9d743d9237ac13e9b397af53cc0ad78c11a8b3b736cc\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk sacremoses default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, nltk, webencodings, html5lib, sentencepiece, sacremoses, tokenizers, filelock, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 filelock-3.0.12 html5lib-1.1 nltk-3.5 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.9.4 transformers-4.2.2 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 21.0.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-01-31 17:09:44,592 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 5,\n",
      "        \"num_labels\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-01-31-17-03-29-449\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-018424395060/pytorch-training-2021-01-31-17-03-29-449/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"num_labels\":3}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-018424395060/pytorch-training-2021-01-31-17-03-29-449/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"num_labels\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-01-31-17-03-29-449\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-018424395060/pytorch-training-2021-01-31-17-03-29-449/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--num_labels\",\"3\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_deploy.py --epochs 5 --num_labels 3\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 33.6MB/s]\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 433/433 [00:00<00:00, 455kB/s]\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   1%|          | 5.50M/440M [00:00<00:07, 55.0MB/s]#015Downloading:   3%|▎         | 11.1M/440M [00:00<00:07, 55.3MB/s]#015Downloading:   4%|▍         | 16.9M/440M [00:00<00:07, 56.0MB/s]#015Downloading:   5%|▌         | 22.7M/440M [00:00<00:07, 56.6MB/s]#015Downloading:   6%|▋         | 28.2M/440M [00:00<00:07, 56.0MB/s]#015Downloading:   8%|▊         | 34.1M/440M [00:00<00:07, 57.0MB/s]#015Downloading:   9%|▉         | 40.1M/440M [00:00<00:06, 57.9MB/s]#015Downloading:  10%|█         | 46.1M/440M [00:00<00:06, 58.4MB/s]#015Downloading:  12%|█▏        | 52.1M/440M [00:00<00:06, 58.9MB/s]#015Downloading:  13%|█▎        | 57.9M/440M [00:01<00:06, 58.7MB/s]#015Downloading:  14%|█▍        | 63.8M/440M [00:01<00:06, 58.7MB/s]#015Downloading:  16%|█▌        | 69.7M/440M [00:01<00:06, 59.0MB/s]#015Downloading:  17%|█▋        | 75.5M/440M [00:01<00:06, 56.1MB/s]#015Downloading:  19%|█▊        | 81.5M/440M [00:01<00:06, 57.2MB/s]#015Downloading:  20%|█▉        | 87.3M/440M [00:01<00:06, 56.5MB/s]#015Downloading:  21%|██        | 93.2M/440M [00:01<00:06, 57.4MB/s]#015Downloading:  22%|██▏       | 99.1M/440M [00:01<00:05, 57.7MB/s]#015Downloading:  24%|██▍       | 105M/440M [00:01<00:05, 58.5MB/s] #015Downloading:  25%|██▌       | 111M/440M [00:01<00:05, 59.1MB/s]#015Downloading:  27%|██▋       | 117M/440M [00:02<00:05, 59.3MB/s]#015Downloading:  28%|██▊       | 123M/440M [00:02<00:05, 59.7MB/s]#015Downloading:  29%|██▉       | 129M/440M [00:02<00:05, 59.9MB/s]#015Downloading:  31%|███       | 135M/440M [00:02<00:05, 59.9MB/s]#015Downloading:  32%|███▏      | 141M/440M [00:02<00:05, 59.3MB/s]#015Downloading:  33%|███▎      | 147M/440M [00:02<00:05, 57.4MB/s]#015Downloading:  35%|███▍      | 153M/440M [00:02<00:04, 57.7MB/s]#015Downloading:  36%|███▌      | 159M/440M [00:02<00:04, 58.4MB/s]#015Downloading:  37%|███▋      | 165M/440M [00:02<00:04, 59.0MB/s]#015Downloading:  39%|███▉      | 171M/440M [00:02<00:04, 59.3MB/s]#015Downloading:  40%|████      | 177M/440M [00:03<00:04, 59.6MB/s]#015Downloading:  42%|████▏     | 183M/440M [00:03<00:04, 59.4MB/s]#015Downloading:  43%|████▎     | 189M/440M [00:03<00:04, 59.6MB/s]#015Downloading:  44%|████▍     | 195M/440M [00:03<00:04, 59.6MB/s]#015Downloading:  46%|████▌     | 201M/440M [00:03<00:04, 59.3MB/s]#015Downloading:  47%|████▋     | 207M/440M [00:03<00:04, 58.0MB/s]#015Downloading:  48%|████▊     | 213M/440M [00:03<00:03, 58.4MB/s]#015Downloading:  50%|████▉     | 219M/440M [00:03<00:03, 58.8MB/s]#015Downloading:  51%|█████     | 225M/440M [00:03<00:03, 59.3MB/s]#015Downloading:  52%|█████▏    | 231M/440M [00:03<00:03, 59.5MB/s]#015Downloading:  54%|█████▍    | 237M/440M [00:04<00:03, 59.7MB/s]#015Downloading:  55%|█████▌    | 243M/440M [00:04<00:03, 60.0MB/s]#015Downloading:  57%|█████▋    | 249M/440M [00:04<00:03, 60.1MB/s]#015Downloading:  58%|█████▊    | 255M/440M [00:04<00:03, 60.1MB/s]#015Downloading:  59%|█████▉    | 261M/440M [00:04<00:02, 60.0MB/s]#015Downloading:  61%|██████    | 267M/440M [00:04<00:03, 56.4MB/s]#015Downloading:  62%|██████▏   | 273M/440M [00:04<00:03, 52.0MB/s]#015Downloading:  63%|██████▎   | 279M/440M [00:04<00:02, 54.1MB/s]#015Downloading:  65%|██████▍   | 284M/440M [00:04<00:02, 54.2MB/s]#015Downloading:  66%|██████▌   | 290M/440M [00:04<00:02, 54.1MB/s]#015Downloading:  67%|██████▋   | 295M/440M [00:05<00:02, 54.3MB/s]#015Downloading:  68%|██████▊   | 301M/440M [00:05<00:02, 55.9MB/s]#015Downloading:  70%|██████▉   | 307M/440M [00:05<00:02, 57.0MB/s]#015Downloading:  71%|███████   | 313M/440M [00:05<00:02, 57.8MB/s]#015Downloading:  72%|███████▏  | 319M/440M [00:05<00:02, 58.4MB/s]#015Downloading:  74%|███████▍  | 325M/440M [00:05<00:02, 51.8MB/s]#015Downloading:  75%|███████▍  | 330M/440M [00:05<00:02, 47.5MB/s]#015Downloading:  76%|███████▌  | 336M/440M [00:05<00:02, 49.5MB/s]#015Downloading:  77%|███████▋  | 341M/440M [00:05<00:02, 48.8MB/s]#015Downloading:  79%|███████▊  | 346M/440M [00:06<00:01, 48.3MB/s]#015Downloading:  80%|███████▉  | 351M/440M [00:06<00:01, 50.1MB/s]#015Downloading:  81%|████████  | 357M/440M [00:06<00:01, 51.6MB/s]#015Downloading:  82%|████████▏ | 362M/440M [00:06<00:01, 49.9MB/s]#015Downloading:  84%|████████▎ | 368M/440M [00:06<00:01, 52.1MB/s]#015Downloading:  85%|████████▍ | 373M/440M [00:06<00:01, 52.8MB/s]#015Downloading:  86%|████████▌ | 379M/440M [00:06<00:01, 53.6MB/s]#015Downloading:  87%|████████▋ | 384M/440M [00:06<00:01, 54.2MB/s]#015Downloading:  89%|████████▊ | 390M/440M [00:06<00:00, 54.0MB/s]#015Downloading:  90%|████████▉ | 396M/440M [00:07<00:00, 54.5MB/s]#015Downloading:  91%|█████████ | 401M/440M [00:07<00:00, 54.8MB/s]#015Downloading:  92%|█████████▏| 407M/440M [00:07<00:00, 55.1MB/s]#015Downloading:  94%|█████████▎| 412M/440M [00:07<00:00, 55.5MB/s]#015Downloading:  95%|█████████▍| 418M/440M [00:07<00:00, 55.5MB/s]#015Downloading:  96%|█████████▌| 423M/440M [00:07<00:00, 55.4MB/s]#015Downloading:  97%|█████████▋| 429M/440M [00:07<00:00, 52.0MB/s]#015Downloading:  99%|█████████▊| 434M/440M [00:07<00:00, 47.2MB/s]#015Downloading: 100%|█████████▉| 439M/440M [00:07<00:00, 47.8MB/s]#015Downloading: 100%|██████████| 440M/440M [00:07<00:00, 55.8MB/s]\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34mProcesses 37500/37500 (100%) of train data\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34mProcesses 12500/12500 (100%) of test data\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "\n",
      "  File \"train_deploy.py\", line 342, in <module>\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:12.969 algo-1:54 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "    train(parser.parse_args())\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:12.969 algo-1:54 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "  File \"train_deploy.py\", line 187, in train\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:12.970 algo-1:54 INFO hook.py:197] Saving to /opt/ml/output/tensors\n",
      "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:12.998 algo-1:54 INFO hook.py:326] Monitoring the collections: losses\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.793 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\n",
      "    result = self.forward(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.793 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 150, in forward\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.793 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\n",
      "    return self.module(*inputs[0], **kwargs[0])\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.794 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.794 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self bool\n",
      "    result = self.forward(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.795 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 1497, in forward\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.804 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\n",
      "    return_dict=return_dict,\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.805 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.805 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\n",
      "    result = self.forward(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.805 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 968, in forward\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.805 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 bool\n",
      "    return_dict=return_dict,\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.814 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.814 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\n",
      "    result = self.forward(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.814 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 566, in forward\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.814 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\n",
      "    output_attentions,\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.814 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self bool\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.815 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\n",
      "    result = self.forward(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.820 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 460, in forward\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.820 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\n",
      "    past_key_value=self_attn_past_key_value,\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.820 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.821 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\n",
      "    result = self.forward(*input, **kwargs)\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.821 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 bool\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 393, in forward\u001b[0m\n",
      "\u001b[34m[2021-01-31 17:12:13.898 algo-1:54 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\n",
      "    output_attentions,\u001b[0m\n",
      "\u001b[34m  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 290, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\u001b[0m\n",
      "\u001b[34mRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.84 GiB already allocated; 46.44 MiB free; 53.14 MiB cached)\u001b[0m\n",
      "\u001b[34m2021-01-31 17:12:14,687 sagemaker-containers ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python train_deploy.py --epochs 5 --num_labels 3\"\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 33.6MB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]#015Downloading: 100%|ââââââââââ| 433/433 [00:00<00:00, 455kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   1%|          | 5.50M/440M [00:00<00:07, 55.0MB/s]#015Downloading:   3%|â         | 11.1M/440M [00:00<00:07, 55.3MB/s]#015Downloading:   4%|â         | 16.9M/440M [00:00<00:07, 56.0MB/s]#015Downloading:   5%|â         | 22.7M/440M [00:00<00:07, 56.6MB/s]#015Downloading:   6%|â         | 28.2M/440M [00:00<00:07, 56.0MB/s]#015Downloading:   8%|â         | 34.1M/440M [00:00<00:07, 57.0MB/s]#015Downloading:   9%|â         | 40.1M/440M [00:00<00:06, 57.9MB/s]#015Downloading:  10%|â         | 46.1M/440M [00:00<00:06, 58.4MB/s]#015Downloading:  12%|ââ        | 52.1M/440M [00:00<00:06, 58.9MB/s]#015Downloading:  13%|ââ        | 57.9M/440M [00:01<00:06, 58.7MB/s]#015Downloading:  14%|ââ        | 63.8M/440M [00:01<00:06, 58.7MB/s]#015Downloading:  16%|ââ        | 69.7M/440M [00:01<00:06, 59.0MB/s]#015Downloading:  17%|ââ        | 75.5M/440M [00:01<00:06, 56.1MB/s]#015Downloading:  19%|ââ        | 81.5M/440M [00:01<00:06, 57.2MB/s]#015Downloading:  20%|ââ        | 87.3M/440M [00:01<00:06, 56.5MB/s]#015Downloading:  21%|ââ        | 93.2M/440M [00:01<00:06, 57.4MB/s]#015Downloading:  22%|âââ       | 99.1M/440M [00:01<00:05, 57.7MB/s]#015Downloading:  24%|âââ       | 105M/440M [00:01<00:05, 58.5MB/s] #015Downloading:  25%|âââ       | 111M/440M [00:01<00:05, 59.1MB/s]#015Downloading:  27%|âââ       | 117M/440M [00:02<00:05, 59.3MB/s]#015Downloading:  28%|âââ       | 123M/440M [00:02<00:05, 59.7MB/s]#015Downloading:  29%|âââ       | 129M/440M [00:02<00:05, 59.9MB/s]#015Downloading:  31%|âââ       | 135M/440M [00:02<00:05, 59.9MB/s]#015Downloading:  32%|ââââ      | 141M/440M [00:02<00:05, 59.3MB/s]#015Downloading:  33%|ââââ      | 147M/440M [00:02<00:05, 57.4MB/s]#015Downloading:  35%|ââââ      | 153M/440M [00:02<00:04, 57.7MB/s]#015Downloading:  36%|ââââ      | 159M/440M [00:02<00:04, 58.4MB/s]#015Downloading:  37%|ââââ      | 165M/440M [00:02<00:04, 59.0MB/s]#015Downloading:  39%|ââââ      | 171M/440M [00:02<00:04, 59.3MB/s]#015Downloading:  40%|ââââ      | 177M/440M [00:03<00:04, 59.6MB/s]#015Downloading:  42%|âââââ     | 183M/440M [00:03<00:04, 59.4MB/s]#015Downloading:  43%|âââââ     | 189M/440M [00:03<00:04, 59.6MB/s]#015Downloading:  44%|âââââ     | 195M/440M [00:03<00:04, 59.6MB/s]#015Downloading:  46%|âââââ     | 201M/440M [00:03<00:04, 59.3MB/s]#015Downloading:  47%|âââââ     | 207M/440M [00:03<00:04, 58.0MB/s]#015Downloading:  48%|âââââ     | 213M/440M [00:03<00:03, 58.4MB/s]#015Downloading:  50%|âââââ     | 219M/440M [00:03<00:03, 58.8MB/s]#015Downloading:  51%|âââââ     | 225M/440M [00:03<00:03, 59.3MB/s]#015Downloading:  52%|ââââââ    | 231M/440M [00:03<00:03, 59.5MB/s]#015Downloading:  54%|ââââââ    | 237M/440M [00:04<00:03, 59.7MB/s]#015Downloading:  55%|ââââââ    | 243M/440M [00:04<00:03, 60.0MB/s]#015Downloading:  57%|ââââââ    | 249M/440M [00:04<00:03, 60.1MB/s]#015Downloading:  58%|ââââââ    | 255M/440M [00:04<00:03, 60.1MB/s]#015Downloading:  59%|ââââââ    | 261M/440M [00:04<00:02, 60.0MB/s]#015Downloading:  61%|ââââââ    | 267M/440M [00:04<00:03, 56.4MB/s]#015Downloading:  62%|âââââââ   | 273M/440M [00:04<00:03, 52.0MB/s]#015Downloading:  63%|âââââââ   | 279M/440M [00:04<00:02, 54.1MB/s]#015Downloading:  65%|âââââââ   | 284M/440M [00:04<00:02, 54.2MB/s]#015Downloading:  66%|âââââââ   | 290M/440M [00:04<00:02, 54.1MB/s]#015Downloading:  67%|âââââââ   | 295M/440M [00:05<00:02, 54.3MB/s]#015Downloading:  68%|âââââââ   | 301M/440M [00:05<00:02, 55.9MB/s]#015Downloading:  70%|âââââââ   | 307M/440M [00:05<00:02, 57.0MB/s]#015Downloading:  71%|âââââââ   | 313M/440M [00:05<00:02, 57.8MB/s]#015Downloading:  72%|ââââââââ  | 319M/440M [00:05<00:02, 58.4MB/s]#015Downloading:  74%|ââââââââ  | 325M/440M [00:05<00:02, 51.8MB/s]#015Downloading:  75%|ââââââââ  | 330M/440M [00:05<00:02, 47.5MB/s]#015Downloading:  76%|ââââââââ  | 336M/440M [00:05<00:02, 49.5MB/s]#015Downloading:  77%|ââââââââ  | 341M/440M [00:05<00:02, 48.8MB/s]#015Downloading:  79%|ââââââââ  | 346M/440M [00:06<00:01, 48.3MB/s]#015Downloading:  80%|ââââââââ  | 351M/440M [00:06<00:01, 50.1MB/s]#015Downloading:  81%|ââââââââ  | 357M/440M [00:06<00:01, 51.6MB/s]#015Downloading:  82%|âââââââââ | 362M/440M [00:06<00:01, 49.9MB/s]#015Downloading:  84%|âââââââââ | 368M/440M [00:06<00:01, 52.1MB/s]#015Downloading:  85%|âââââââââ | 373M/440M [00:06<00:01, 52.8MB/s]#015Downloading:  86%|âââââââââ | 379M/440M [00:06<00:01, 53.6MB/s]#015Downloading:  87%|âââââââââ | 384M/440M [00:06<00:01, 54.2MB/s]#015Downloading:  89%|âââââââââ | 390M/440M [00:06<00:00, 54.0MB/s]#015Downloading:  90%|âââââââââ | 396M/440M [00:07<00:00, 54.5MB/s]#015Downloading:  91%|âââââââââ | 401M/440M [00:07<00:00, 54.8MB/s]#015Downloading:  92%|ââââââââââ| 407M/440M [00:07<00:00, 55.1MB/s]#015Downloading:  94%|ââââââââââ| 412M/440M [00:07<00:00, 55.5MB/s]#015Downloading:  95%|ââââââââââ| 418M/440M [00:07<00:00, 55.5MB/s]#015Downloading:  96%|ââââââââââ| 423M/440M [00:07<00:00, 55.4MB/s]#015Downloading:  97%|ââââââââââ| 429M/440M [00:07<00:00, 52.0MB/s]#015Downloading:  99%|ââââââââââ| 434M/440M [00:07<00:00, 47.2MB/s]#015Downloading: 100%|ââââââââââ| 439M/440M [00:07<00:00, 47.8MB/s]#015Downloading: 100%|ââââââââââ| 440M/440M [00:07<00:00, 55.8MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train_deploy.py\", line 342, in <module>\n",
      "    train(parser.parse_args())\n",
      "  File \"train_deploy.py\", line 187, in train\n",
      "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 150, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 1497, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 968, in forward\n",
      "    return_dict=return_dict,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 566, in forward\n",
      "    output_attentions,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 460, in forward\n",
      "    past_key_value=self_attn_past_key_value,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 393, in forward\n",
      "    output_attentions,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 552, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/models/bert/modeling_bert.py\", line 290, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\u001b[0m\n",
      "\u001b[34mRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 11.17 GiB total capacity; 10.84 GiB already allocated; 46.44 MiB free; 53.14 MiB cached)\u001b[0m\n",
      "\n",
      "2021-01-31 17:12:24 Uploading - Uploading generated training model\n",
      "2021-01-31 17:12:24 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2021-01-31-17-03-29-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train_deploy.py --epochs 5 --num_labels 3\"\n\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 33.6MB/s]\n\rDownloading:   0%|          | 0.00/433 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 433/433 [00:00<00:00, 455kB/s]\n\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   1%|          | 5.50M/440M [00:00<00:07, 55.0MB/s]\rDownloading:   3%|â         | 11.1M/440M [00:00<00:07, 55.3MB/s]\rDownloading:   4%|â         | 16.9M/440M [00:00<00:07, 56.0MB/s]\rDownloading:   5%|â         | 22.7M/440M [00:00<00:07, 56.6MB/s]\rDownloading:   6%|â         | 28.2M/440M [00:00<00:07, 56.0MB/s]\rDownloading:   8%|â         | 34.1M/440M [00:00<00:07, 57.0MB/s]\rDownloading:   9%|â         | 40.1M/440M [00:00<00:06, 57.9MB/s]\rDownloading:  10%|â         | 46.1M/440M [00:00<00:06, 58.4MB/s]\rDownloading:  12%|ââ        | 52.1M/4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7eb47513043c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     }\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_validation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3076\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3077\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3078\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                 ),\n\u001b[1;32m   2670\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             )\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2021-01-31-17-03-29-449: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python train_deploy.py --epochs 5 --num_labels 3\"\n\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 232k/232k [00:00<00:00, 33.6MB/s]\n\rDownloading:   0%|          | 0.00/433 [00:00<?, ?B/s]\rDownloading: 100%|ââââââââââ| 433/433 [00:00<00:00, 455kB/s]\n\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   1%|          | 5.50M/440M [00:00<00:07, 55.0MB/s]\rDownloading:   3%|â         | 11.1M/440M [00:00<00:07, 55.3MB/s]\rDownloading:   4%|â         | 16.9M/440M [00:00<00:07, 56.0MB/s]\rDownloading:   5%|â         | 22.7M/440M [00:00<00:07, 56.6MB/s]\rDownloading:   6%|â         | 28.2M/440M [00:00<00:07, 56.0MB/s]\rDownloading:   8%|â         | 34.1M/440M [00:00<00:07, 57.0MB/s]\rDownloading:   9%|â         | 40.1M/440M [00:00<00:06, 57.9MB/s]\rDownloading:  10%|â         | 46.1M/440M [00:00<00:06, 58.4MB/s]\rDownloading:  12%|ââ        | 52.1M/4"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# place to save model artifact\n",
    "output_path = \"s3://{}/{}\".format(bucket, prefix)\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\",\n",
    "    source_dir=\"train\",\n",
    "    role=role,\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p2.xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 5,\n",
    "        \"num_labels\": 3\n",
    "    }\n",
    ")\n",
    "estimator.fit({\"training\": input_train, \"testing\": input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy our endpoint, we call deploy() on our PyTorch estimator object, passing in our desired number of instances and instance type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the predictor to use application/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(training_data_frame[0]['text'])\n",
    "print(\"predicted class: \", np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model for testing\n",
    "\n",
    "Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model.to(device)\n",
    "criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert conversion of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic operations to understand how BERT converts a sentence into tokens and then into IDs\n",
    "sample_body = 'script stopped adding videos saenzramiro abc xyz'\n",
    "tokens = tokenizer.tokenize(sample_body)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_body}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding [CLS]:101, [SEP]:102, [PAD]:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using encode_plus to add special tokens : [CLS]:101, [SEP]:102, [PAD]:0\n",
    "encodings = tokenizer.encode_plus(\n",
    "            sample_body,\n",
    "            max_length=32,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    ")\n",
    "\n",
    "encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Input IDs : {}'.format(encodings['input_ids'][0]))\n",
    "print('\\nAttention Mask : {}'.format(encodings['attention_mask'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a Class for GitHub Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Git_Message(Dataset):\n",
    "    def __init__(self, git_messages, label, tokenizer, max_len):\n",
    "        self.git_messages = git_messages\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.git_messages)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        git_messages = str(self.git_messages[item])\n",
    "        label = self.label[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        git_messages,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt')\n",
    "        return {\n",
    "        'git_messages': git_messages,\n",
    "         'input_ids': encoding['input_ids'],\n",
    "         'attention_mask': encoding['attention_mask'],\n",
    "         'label': torch.tensor(label, dtype=torch.long)\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train, Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_data_frame.head()\n",
    "training_data_frame = training_data_frame[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(\n",
    "    training_data_frame,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "testing_data, validation_data = train_test_split(\n",
    "    testing_data,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape, testing_data.shape, validation_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data, tokenizer, max_len, batch_size):\n",
    "    \n",
    "    ds = Git_Message(git_messages=data.text.to_numpy(),\n",
    "    label=data.label.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len)\n",
    "    \n",
    "    return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(training_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "testing_data_loader = create_data_loader(testing_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(validation_data, tokenizer, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_frame = next(iter(train_data_loader))\n",
    "training_data_frame.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_frame['input_ids'].squeeze().shape, training_data_frame['attention_mask'].squeeze().shape, training_data_frame['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('git_messages  : ', training_data_frame['git_messages'][0])\n",
    "print('input_ids : ', training_data_frame['input_ids'].squeeze()[0])\n",
    "print('attention_mask : ', training_data_frame['attention_mask'].squeeze()[0])\n",
    "print('label : ', training_data_frame['label'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "  input_ids=encodings['input_ids'],\n",
    "  attention_mask=encodings['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state.shape, pooled_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BugPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(BugPredictor, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.out = nn.Linear(self.bert_model.config.hidden_size, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask = attention_mask\n",
    "        )\n",
    "        output = self.dropout(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Start Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "label 0: Bug\n",
    "label 1: Feature\n",
    "label 2: Question\n",
    "\"\"\"\n",
    "class_names = [0, 1, 2]\n",
    "bug_predictor_model = BugPredictor(len(class_names))\n",
    "bug_predictor_model = bug_predictor_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(bug_predictor_model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d['input_ids'].squeeze().to(device)\n",
    "        attention_mask = d['attention_mask'].squeeze().to(device)\n",
    "        targets = d['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        correct_predictions += torch.sum(preds==targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d['input_ids'].squeeze().to(device)\n",
    "            attention_mask = d['attention_mask'].squeeze().to(device)\n",
    "            targets = d['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds==targets)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return correct_predictions.double()/n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}/{}'.format(epoch+1,EPOCHS))\n",
    "    print('-' * 10)\n",
    "  \n",
    "    train_acc, train_loss = train_model(bug_predictor_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(training_data))\n",
    "    \n",
    "    print('Train loss : {} accuracy : {}'.format(train_loss, train_acc))\n",
    "    \n",
    "    val_acc, val_loss = eval_model(bug_predictor_model, val_data_loader, loss_fn, device, len(validation_data))\n",
    "    \n",
    "    print('Validation loss : {} accuracy : {}'.format(val_loss, val_acc))\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        print('Saving the best model ...')\n",
    "        torch.save(bug_predictor_model.state_dict(), 'best_model.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bug_message = \"Script stopped adding video's. A recent change in the youtube layout broke the script. Probably caused by element names being altered.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['bug', 'feature', 'question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_git_category(sample_message, model):\n",
    "    encoded_message = tokenizer.encode_plus(sample_bug_message, max_length=MAX_LENGTH, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids = encoded_message['input_ids'].to(device)\n",
    "    attention_mask = encoded_message['attention_mask'].to(device)\n",
    "    \n",
    "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    _, prediction_idx = torch.max(output, dim=1)\n",
    "        \n",
    "    return class_names[prediction_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sample bug message : ', sample_bug_message)\n",
    "print('Predicted GitHub Category : ', predict_git_category(sample_bug_message, bug_predictor_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
