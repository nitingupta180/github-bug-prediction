{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Bug Prediction with Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of this project is to apply Deep Learning NLP based techniques to predict the bugs, features, and questions based on GitHub titles and the text body.\n",
    "\n",
    "We need to predict\n",
    "\n",
    "    ● Bug - 0\n",
    "    ● Feature - 1\n",
    "    ● Question - 2\n",
    "given Github titles and text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install sagemaker, kaggle and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.8)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.16.63)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.63 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.63)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.63->boto3>=1.14.12->sagemaker==1.72.0) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.63->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: kaggle in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.5.10)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (4.42.1)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: urllib3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (1.26.2)\n",
      "Requirement already satisfied: six>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: python-slugify in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (4.0.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2.25.1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->kaggle) (4.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.3.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0\n",
    "!pip install kaggle\n",
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries which will be required in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from tqdm.autonotebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import sagemaker related libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create directory named kaggle which will have json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘.kaggle’: File exists\n"
     ]
    }
   ],
   "source": [
    "%mkdir .kaggle\n",
    "!touch .kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create kaggle.json file which will have api key to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: inter-device move failed: ‘.kaggle’ to ‘/home/ec2-user/.kaggle’; unable to remove target: Directory not empty\n"
     ]
    }
   ],
   "source": [
    "api_token = {\"username\":\"codewarrior0\",\"key\":\"c01b65090e3a59a881fd19675d7a54e1\"}\n",
    "\n",
    "with open('/home/ec2-user/SageMaker/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 /home/ec2-user/SageMaker/.kaggle/kaggle.json\n",
    "%mv .kaggle /home/ec2-user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create data directory named kaggle which will have all the data downloaded from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading github-bugs-prediction.zip to /home/ec2-user/SageMaker\n",
      " 94%|███████████████████████████████████▌  | 92.0M/98.3M [00:01<00:00, 92.0MB/s]\n",
      "100%|██████████████████████████████████████| 98.3M/98.3M [00:01<00:00, 92.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d anmolkumar/github-bugs-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  github-bugs-prediction.zip\n",
      "  inflating: embold_test.json        \n",
      "  inflating: embold_train.json       \n",
      "  inflating: embold_train_extra.json  \n",
      "  inflating: sample submission.csv   \n"
     ]
    }
   ],
   "source": [
    "!unzip github-bugs-prediction.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move train and test data to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mv embold_train.json kaggle/train.json\n",
    "%mv embold_test.json kaggle/test.json\n",
    "%mv embold_train_extra.json kaggle/train_extra.json\n",
    "%rm github-bugs-prediction.zip\n",
    "%rm sample\\ submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data in a dataframe and see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \n",
       "0        a y-zoom on the piano roll would be useful.      1  \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0  \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1  \n",
       "3  i think we should stop logging requests to:\\r ...      1  \n",
       "4  expected behavior\\r alarm actions pid on and p...      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= pd.read_json(\"./kaggle/train.json\").reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data in dataframe and see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>config question  path-specific environment var...</td>\n",
       "      <td>issue description or question\\r \\r hey @artemg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crash indien vol</td>\n",
       "      <td>de simulator crasht als hij vol zit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unable to mine rocks</td>\n",
       "      <td>sarkasmo starting today, when i hit enter  act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not all whitelists are processed</td>\n",
       "      <td>create following rules... order of creation is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>add ctx menu for idafree 70 and idafree 5</td>\n",
       "      <td>associated with .dll, .dll_, .exe, .exe_, .sc,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  config question  path-specific environment var...   \n",
       "1                                   crash indien vol   \n",
       "2                               unable to mine rocks   \n",
       "3                   not all whitelists are processed   \n",
       "4          add ctx menu for idafree 70 and idafree 5   \n",
       "\n",
       "                                                body  \n",
       "0  issue description or question\\r \\r hey @artemg...  \n",
       "1                de simulator crasht als hij vol zit  \n",
       "2  sarkasmo starting today, when i hit enter  act...  \n",
       "3  create following rules... order of creation is...  \n",
       "4  associated with .dll, .dll_, .exe, .exe_, .sc,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df= pd.read_json(\"./kaggle/test.json\").reset_index(drop=True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load extra training data in dataframe and see the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>use a 8bit typeface</td>\n",
       "      <td>since this is meant to emulate some old arcade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>implement wireless m-bus binding</td>\n",
       "      <td>_from  chris.pa...@googlemail.com  https://cod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>add multilang support for timeago.js</td>\n",
       "      <td>currently it is only  en . \\r required to add ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scaleway - seg-fault on shutdown</td>\n",
       "      <td>tbr  irc  creates a new scaleway instance with...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sistema de pintura: no se guardar los nuevos p...</td>\n",
       "      <td>este sp ya estaba asignado a un carro y se enc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                use a 8bit typeface   \n",
       "1                   implement wireless m-bus binding   \n",
       "2               add multilang support for timeago.js   \n",
       "3                   scaleway - seg-fault on shutdown   \n",
       "4  sistema de pintura: no se guardar los nuevos p...   \n",
       "\n",
       "                                                body  label  \n",
       "0  since this is meant to emulate some old arcade...      1  \n",
       "1  _from  chris.pa...@googlemail.com  https://cod...      1  \n",
       "2  currently it is only  en . \\r required to add ...      1  \n",
       "3  tbr  irc  creates a new scaleway instance with...      0  \n",
       "4  este sp ya estaba asignado a un carro y se enc...      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_extra_df= pd.read_json(\"./kaggle/train_extra.json\").reset_index(drop=True)\n",
    "train_extra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in train.json and train_extra.json is similar. Combine the data in both files as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 150000\n",
      "300000 300000\n",
      "30000 30000\n"
     ]
    }
   ],
   "source": [
    "def dataset_length_check(data_frame):\n",
    "    print(len(data_frame),data_frame.index.shape[-1])\n",
    "                 \n",
    "dataset_length_check(train_df)\n",
    "dataset_length_check(train_extra_df)\n",
    "dataset_length_check(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \n",
       "0        a y-zoom on the piano roll would be useful.      1  \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0  \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1  \n",
       "3  i think we should stop logging requests to:\\r ...      1  \n",
       "4  expected behavior\\r alarm actions pid on and p...      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame = train_df[:10000]\n",
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the combiled length of training data. It should be 450000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   10000 non-null  object\n",
      " 1   body    10000 non-null  object\n",
      " 2   label   10000 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "training_data_frame.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets categorize the data based on labels and name them as df_bug, df_feature and df_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Counts of label column: \n",
      " 1    4550\n",
      "0    4498\n",
      "2     952\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa65bf3b5c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Total Counts of label column: \\n'.format(),training_data_frame['label'].value_counts())\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "sns.countplot(x='label', data=training_data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bug = training_data_frame[training_data_frame['label']==0]\n",
    "df_feature = training_data_frame[training_data_frame['label']==1]\n",
    "df_question = training_data_frame[training_data_frame['label']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4498\n",
       "1    4550\n",
       "2     952\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = training_data_frame.label.value_counts().sort_index()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine the text in title and body and create new column with name text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(x):\n",
    "    return x['title'] + \" \" + x['body']   \n",
    "training_data_frame['text']= training_data_frame.apply(lambda row : combine(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints with label as Bug : 4498\n",
      "Number of datapoints with label as Feature : 4550\n",
      "Number of datapoints with label as Question : 952\n"
     ]
    }
   ],
   "source": [
    "print('Number of datapoints with label as Bug :',label_counts[0])\n",
    "print('Number of datapoints with label as Feature :',label_counts[1])\n",
    "print('Number of datapoints with label as Question :',label_counts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>y-zoom piano roll a y-zoom on the piano roll w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection ! screenshot from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi,\\r \\r great job so far,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  y-zoom piano roll a y-zoom on the piano roll w...  \n",
       "1  buggy behavior in selection ! screenshot from ...  \n",
       "2  auto update feature hi,\\r \\r great job so far,...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on / pid off alarm actions for ardu...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing/Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ensure we make text lowercase, remove text in square brackets, remove links, remove punctuation and remove words containing numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords & Punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(git_text):\n",
    "    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n",
    "    # remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n",
    "    # convert them back to sentences and split into words\n",
    "    # remove_punctuation = \"\".join(remove_punctuation).split()\n",
    "    filtered_git_text = [word.lower() for word in git_text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_git_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "\n",
    "def visulize_dataset(data_frame, category):\n",
    "    \n",
    "    # Let's apply the above two functions 'clean_text' and 'remove_stopwords' to the whole dataset\n",
    "\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "    data_frame['text'] = data_frame['text'].apply(lambda x: clean_text(x))\n",
    "    data_frame[\"text\"] = data_frame[\"text\"].apply(remove_stopwords)\n",
    "    \n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for i, j in data_frame.iterrows():\n",
    "        for word in j['text']:\n",
    "            word_list.append(word)\n",
    "        \n",
    "    count_dict = Counter(word_list)\n",
    "    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n",
    "    fig = px.histogram(most_common_words_df,\n",
    "                       x='word', \n",
    "                       y='count',\n",
    "                       title='Most common terms used while refering to a GitHub {}'.format(category),\n",
    "                       color_discrete_sequence=['#843B62'] )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing the text field after Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y-zoom piano roll</td>\n",
       "      <td>a y-zoom on the piano roll would be useful.</td>\n",
       "      <td>1</td>\n",
       "      <td>yzoom piano roll a yzoom on the piano roll wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buggy behavior in selection</td>\n",
       "      <td>! screenshot from 2016-02-23 21 27 40  https:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>buggy behavior in selection  screenshot from  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>auto update feature</td>\n",
       "      <td>hi,\\r \\r great job so far, @saenzramiro ! : \\r...</td>\n",
       "      <td>1</td>\n",
       "      <td>auto update feature hi  great job so far saenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>filter out noisy endpoints in logs</td>\n",
       "      <td>i think we should stop logging requests to:\\r ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filter out noisy endpoints in logs i think we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enable pid on / pid off alarm actions for ardu...</td>\n",
       "      <td>expected behavior\\r alarm actions pid on and p...</td>\n",
       "      <td>0</td>\n",
       "      <td>enable pid on  pid off alarm actions for  expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                  y-zoom piano roll   \n",
       "1                        buggy behavior in selection   \n",
       "2                                auto update feature   \n",
       "3                 filter out noisy endpoints in logs   \n",
       "4  enable pid on / pid off alarm actions for ardu...   \n",
       "\n",
       "                                                body  label  \\\n",
       "0        a y-zoom on the piano roll would be useful.      1   \n",
       "1  ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2  hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3  i think we should stop logging requests to:\\r ...      1   \n",
       "4  expected behavior\\r alarm actions pid on and p...      0   \n",
       "\n",
       "                                                text  \n",
       "0  yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1  buggy behavior in selection  screenshot from  ...  \n",
       "2  auto update feature hi  great job so far saenz...  \n",
       "3  filter out noisy endpoints in logs i think we ...  \n",
       "4  enable pid on  pid off alarm actions for  expe...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "label 0: Bug\n",
    "label 1: Feature\n",
    "label 2: Question\n",
    "\"\"\"\n",
    "training_data_frame['text'] = training_data_frame['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "training_data_frame['text'] = training_data_frame['text'].apply(lambda x: clean_text(x))\n",
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remove stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_frame['text'] = training_data_frame[\"text\"].apply(remove_stopwords)\n",
    "training_data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                   title  \\\n",
       "0                                     y-zoom piano roll   \n",
       "1                           buggy behavior in selection   \n",
       "2                                   auto update feature   \n",
       "3                    filter out noisy endpoints in logs   \n",
       "4     enable pid on / pid off alarm actions for ardu...   \n",
       "...                                                 ...   \n",
       "9995  planned event bug: \\ page not found\\  as regul...   \n",
       "9996                                idea: tour recorder   \n",
       "9997        extract  add support for parsing jats files   \n",
       "9998  volume store folder name validation in the cre...   \n",
       "9999                                    building on osx   \n",
       "\n",
       "                                                   body  label  \\\n",
       "0           a y-zoom on the piano roll would be useful.      1   \n",
       "1     ! screenshot from 2016-02-23 21 27 40  https:/...      0   \n",
       "2     hi,\\r \\r great job so far, @saenzramiro ! : \\r...      1   \n",
       "3     i think we should stop logging requests to:\\r ...      1   \n",
       "4     expected behavior\\r alarm actions pid on and p...      0   \n",
       "...                                                 ...    ...   \n",
       "9995  i just created the demo and another extra even...      0   \n",
       "9996  overlay, record clicks / click targets, pop up...      1   \n",
       "9997  > the journal article tag suite  jats  is an x...      1   \n",
       "9998  you can enter a volume store folder name conta...      0   \n",
       "9999  in the wiki  compiling  https://github.com/kwh...      0   \n",
       "\n",
       "                                                   text  \n",
       "0     yzoom piano roll a yzoom on the piano roll wou...  \n",
       "1     buggy behavior in selection  screenshot from  ...  \n",
       "2     auto update feature hi  great job so far saenz...  \n",
       "3     filter out noisy endpoints in logs i think we ...  \n",
       "4     enable pid on  pid off alarm actions for  expe...  \n",
       "...                                                 ...  \n",
       "9995  planned event bug  page not found  as regular ...  \n",
       "9996  idea tour recorder overlay record clicks  clic...  \n",
       "9997  extract  add support for parsing jats files  t...  \n",
       "9998  volume store folder name validation in the cre...  \n",
       "9999  building on osx in the wiki  compiling    ther...  \n",
       "\n",
       "[10000 rows x 4 columns]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a directory name data which will hold the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need the text and label column from our data for training. Extract text and label column from data_frame and write it in data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, validation = train_test_split(training_data_frame[['label', 'text']])\n",
    "train.to_csv(\"./data/train.csv\", index = False)\n",
    "validation.to_csv(\"./data/validation.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat same steps for testing data. Clean and preprocess it and write the data in test.csv file in data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining title and body and creating new column with name text\n",
    "test_df['text'] = test_df.apply(lambda row : combine(row),axis=1)\n",
    "# Clean the text column by removing punctuation, stopwords etc\n",
    "test_df['text'] = test_df['text'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n",
    "test_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n",
    "#test_df['text'] = test_df['text'].apply(lambda x: remove_stopwords(x))\n",
    "test_df.head()\n",
    "# Extract text and label column\n",
    "test = test_df[['text']]\n",
    "test.to_csv(\"./data/test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa6911cc970>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get sagemaker session, s3 bucket to upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'mlcapstone/bug_prediction'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = sagemaker_session.upload_data(\"./data/train.csv\", bucket=bucket, key_prefix=prefix)\n",
    "input_validation = sagemaker_session.upload_data(\"./data/validation.csv\", bucket=bucket, key_prefix=prefix)\n",
    "input_test = sagemaker_session.upload_data(\"./data/test.csv\", bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Model on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:35:48 Starting - Starting the training job...\n",
      "2021-03-01 15:35:50 Starting - Launching requested ML instances......\n",
      "2021-03-01 15:37:05 Starting - Preparing the instances for training......\n",
      "2021-03-01 15:38:18 Downloading - Downloading input data...\n",
      "2021-03-01 15:38:38 Training - Downloading the training image.....................\n",
      "2021-03-01 15:42:17 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:19,416 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:19,445 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:20,879 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:21,263 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:21,264 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:21,264 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:21,264 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp2nreisy7/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.25.0)\u001b[0m\n",
      "\u001b[34mCollecting numpy==1.17\n",
      "  Downloading numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4 MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (4.8.2)\u001b[0m\n",
      "\u001b[34mCollecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2019.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from nltk->-r requirements.txt (line 3)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.6/site-packages (from beautifulsoup4->-r requirements.txt (line 4)) (1.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from html5lib->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 7)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 11)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 11)) (20.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from transformers->-r requirements.txt (line 11)) (1.4.0)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers->-r requirements.txt (line 11)) (2.4.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 11)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: more-itertools in /opt/conda/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->transformers->-r requirements.txt (line 11)) (8.1.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, sacremoses, default-user-module-name\n",
      "  Building wheel for nltk (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=6f1ac97e20570ae51ddf67ffc361751f529a9cbe1cd194be91d23b44885de14a\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=83c454bd684706435024854640865dd20f97646682063cdfac12d8517c3a667a\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=25424 sha256=ec0b4e19277e9d145be3e428085fb954f7578c458f3292bed1a1a04e9f3ff04a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-9cwo88oq/wheels/82/c0/23/acd7b6b43240bedf9539a45bf6f7f8bd96a92cb62815ff22ae\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk sacremoses default-user-module-name\u001b[0m\n",
      "\u001b[34mERROR: sagemaker-pytorch-container 1.3.1 has requirement numpy==1.16.4, but you'll have numpy 1.17.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, regex, nltk, webencodings, html5lib, sentencepiece, sacremoses, filelock, tokenizers, transformers, default-user-module-name\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 filelock-3.0.12 html5lib-1.1 nltk-3.5 numpy-1.17.0 regex-2020.11.13 sacremoses-0.0.43 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.3 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 21.0.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-03-01 15:42:35,711 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 8,\n",
      "        \"test_batch_size\": 8,\n",
      "        \"epochs\": 1,\n",
      "        \"num_labels\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2021-03-01-15-35-47-872\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-018424395060/pytorch-training-2021-03-01-15-35-47-872/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":8,\"epochs\":1,\"num_labels\":3,\"test_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-018424395060/pytorch-training-2021-03-01-15-35-47-872/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":8,\"epochs\":1,\"num_labels\":3,\"test_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2021-03-01-15-35-47-872\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-018424395060/pytorch-training-2021-03-01-15-35-47-872/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"8\",\"--epochs\",\"1\",\"--num_labels\",\"3\",\"--test_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_deploy.py --batch_size 8 --epochs 1 --num_labels 3 --test_batch_size 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 35.0MB/s]\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 433/433 [00:00<00:00, 166kB/s]\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   1%|          | 3.66M/440M [00:00<00:11, 36.6MB/s]#015Downloading:   2%|▏         | 7.42M/440M [00:00<00:11, 36.9MB/s]#015Downloading:   2%|▏         | 10.5M/440M [00:00<00:12, 34.7MB/s]#015Downloading:   3%|▎         | 14.1M/440M [00:00<00:12, 35.3MB/s]#015Downloading:   4%|▍         | 17.8M/440M [00:00<00:11, 35.6MB/s]#015Downloading:   5%|▍         | 21.4M/440M [00:00<00:11, 35.8MB/s]#015Downloading:   6%|▌         | 24.9M/440M [00:00<00:11, 35.5MB/s]#015Downloading:   7%|▋         | 28.9M/440M [00:00<00:11, 36.9MB/s]#015Downloading:   7%|▋         | 32.5M/440M [00:00<00:11, 36.4MB/s]#015Downloading:   9%|▊         | 37.8M/440M [00:01<00:10, 40.2MB/s]#015Downloading:  10%|▉         | 43.3M/440M [00:01<00:09, 43.7MB/s]#015Downloading:  11%|█         | 48.8M/440M [00:01<00:08, 46.6MB/s]#015Downloading:  12%|█▏        | 54.0M/440M [00:01<00:08, 48.2MB/s]#015Downloading:  14%|█▎        | 59.5M/440M [00:01<00:07, 50.0MB/s]#015Downloading:  15%|█▍        | 64.6M/440M [00:01<00:07, 50.0MB/s]#015Downloading:  16%|█▌        | 70.2M/440M [00:01<00:07, 51.8MB/s]#015Downloading:  17%|█▋        | 75.9M/440M [00:01<00:06, 53.1MB/s]#015Downloading:  19%|█▊        | 81.5M/440M [00:01<00:06, 54.1MB/s]#015Downloading:  20%|█▉        | 87.2M/440M [00:01<00:06, 54.8MB/s]#015Downloading:  21%|██        | 92.9M/440M [00:02<00:06, 55.4MB/s]#015Downloading:  22%|██▏       | 98.5M/440M [00:02<00:06, 55.7MB/s]#015Downloading:  24%|██▎       | 104M/440M [00:02<00:06, 56.0MB/s] #015Downloading:  25%|██▍       | 110M/440M [00:02<00:05, 56.0MB/s]#015Downloading:  26%|██▌       | 115M/440M [00:02<00:05, 56.1MB/s]#015Downloading:  27%|██▋       | 121M/440M [00:02<00:05, 55.0MB/s]#015Downloading:  29%|██▉       | 127M/440M [00:02<00:05, 55.5MB/s]#015Downloading:  30%|███       | 132M/440M [00:02<00:05, 53.5MB/s]#015Downloading:  31%|███▏      | 138M/440M [00:02<00:05, 52.0MB/s]#015Downloading:  33%|███▎      | 143M/440M [00:02<00:05, 53.1MB/s]#015Downloading:  34%|███▍      | 149M/440M [00:03<00:05, 54.1MB/s]#015Downloading:  35%|███▌      | 155M/440M [00:03<00:05, 54.7MB/s]#015Downloading:  36%|███▋      | 160M/440M [00:03<00:05, 54.6MB/s]#015Downloading:  38%|███▊      | 166M/440M [00:03<00:04, 55.0MB/s]#015Downloading:  39%|███▉      | 171M/440M [00:03<00:04, 55.0MB/s]#015Downloading:  40%|████      | 177M/440M [00:03<00:04, 53.6MB/s]#015Downloading:  41%|████▏     | 182M/440M [00:03<00:04, 54.2MB/s]#015Downloading:  43%|████▎     | 188M/440M [00:03<00:04, 54.9MB/s]#015Downloading:  44%|████▍     | 194M/440M [00:03<00:04, 55.4MB/s]#015Downloading:  45%|████▌     | 199M/440M [00:03<00:04, 55.5MB/s]#015Downloading:  46%|████▋     | 205M/440M [00:04<00:04, 55.8MB/s]#015Downloading:  48%|████▊     | 210M/440M [00:04<00:04, 55.8MB/s]#015Downloading:  49%|████▉     | 216M/440M [00:04<00:04, 55.9MB/s]#015Downloading:  50%|█████     | 222M/440M [00:04<00:03, 55.7MB/s]#015Downloading:  52%|█████▏    | 227M/440M [00:04<00:03, 55.8MB/s]#015Downloading:  53%|█████▎    | 233M/440M [00:04<00:03, 54.0MB/s]#015Downloading:  54%|█████▍    | 238M/440M [00:04<00:03, 54.5MB/s]#015Downloading:  55%|█████▌    | 244M/440M [00:04<00:03, 55.0MB/s]#015Downloading:  57%|█████▋    | 249M/440M [00:04<00:03, 54.8MB/s]#015Downloading:  58%|█████▊    | 255M/440M [00:04<00:03, 55.2MB/s]#015Downloading:  59%|█████▉    | 261M/440M [00:05<00:03, 55.6MB/s]#015Downloading:  60%|██████    | 266M/440M [00:05<00:03, 55.4MB/s]#015Downloading:  62%|██████▏   | 272M/440M [00:05<00:03, 55.6MB/s]#015Downloading:  63%|██████▎   | 277M/440M [00:05<00:02, 54.4MB/s]#015Downloading:  64%|██████▍   | 283M/440M [00:05<00:02, 54.9MB/s]#015Downloading:  66%|██████▌   | 289M/440M [00:05<00:02, 53.7MB/s]#015Downloading:  67%|██████▋   | 294M/440M [00:05<00:02, 54.3MB/s]#015Downloading:  68%|██████▊   | 300M/440M [00:05<00:02, 55.0MB/s]#015Downloading:  69%|██████▉   | 305M/440M [00:05<00:02, 55.4MB/s]#015Downloading:  71%|███████   | 311M/440M [00:05<00:02, 55.6MB/s]#015Downloading:  72%|███████▏  | 317M/440M [00:06<00:02, 55.8MB/s]#015Downloading:  73%|███████▎  | 322M/440M [00:06<00:02, 55.9MB/s]#015Downloading:  74%|███████▍  | 328M/440M [00:06<00:02, 55.9MB/s]#015Downloading:  76%|███████▌  | 333M/440M [00:06<00:01, 54.2MB/s]#015Downloading:  77%|███████▋  | 339M/440M [00:06<00:01, 54.7MB/s]#015Downloading:  78%|███████▊  | 344M/440M [00:06<00:01, 53.2MB/s]#015Downloading:  79%|███████▉  | 350M/440M [00:06<00:01, 53.8MB/s]#015Downloading:  81%|████████  | 356M/440M [00:06<00:01, 54.5MB/s]#015Downloading:  82%|████████▏ | 361M/440M [00:06<00:01, 54.8MB/s]#015Downloading:  83%|████████▎ | 367M/440M [00:06<00:01, 55.2MB/s]#015Downloading:  85%|████████▍ | 372M/440M [00:07<00:01, 55.5MB/s]#015Downloading:  86%|████████▌ | 378M/440M [00:07<00:01, 55.6MB/s]#015Downloading:  87%|████████▋ | 384M/440M [00:07<00:01, 55.7MB/s]#015Downloading:  88%|████████▊ | 389M/440M [00:07<00:00, 55.8MB/s]#015Downloading:  90%|████████▉ | 395M/440M [00:07<00:00, 56.0MB/s]#015Downloading:  91%|█████████ | 400M/440M [00:07<00:00, 54.8MB/s]#015Downloading:  92%|█████████▏| 406M/440M [00:07<00:00, 55.2MB/s]#015Downloading:  93%|█████████▎| 412M/440M [00:07<00:00, 53.2MB/s]#015Downloading:  95%|█████████▍| 417M/440M [00:07<00:00, 49.9MB/s]#015Downloading:  96%|█████████▌| 422M/440M [00:08<00:00, 51.5MB/s]#015Downloading:  97%|█████████▋| 428M/440M [00:08<00:00, 52.7MB/s]#015Downloading:  98%|█████████▊| 434M/440M [00:08<00:00, 53.7MB/s]#015Downloading: 100%|█████████▉| 439M/440M [00:08<00:00, 54.1MB/s]#015Downloading: 100%|██████████| 440M/440M [00:08<00:00, 52.7MB/s]\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34mProcesses 7500/7500 (100%) of train data\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34mProcesses 2500/2500 (100%) of test data\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/7500 (0%)] Loss: 1.072716\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [400/7500 (5%)] Loss: 0.807680\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:20.742 algo-1:54 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [800/7500 (11%)] Loss: 0.872784\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:20.743 algo-1:54 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1200/7500 (16%)] Loss: 0.660071\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:20.743 algo-1:54 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1600/7500 (21%)] Loss: 0.202899\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:20.764 algo-1:54 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2000/7500 (27%)] Loss: 1.543220\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.630 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2400/7500 (32%)] Loss: 0.540362\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.630 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2800/7500 (37%)] Loss: 0.628304\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.630 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/7500 (43%)] Loss: 0.630371\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.630 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3600/7500 (48%)] Loss: 0.678691\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.630 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self bool\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4000/7500 (53%)] Loss: 0.718580\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.631 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4400/7500 (59%)] Loss: 0.970521\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.636 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4800/7500 (64%)] Loss: 0.720250\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.637 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5200/7500 (69%)] Loss: 0.097952\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.637 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5600/7500 (75%)] Loss: 1.449223\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.637 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6000/7500 (80%)] Loss: 0.764409\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.637 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 bool\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/7500 (85%)] Loss: 0.752519\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.641 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6800/7500 (91%)] Loss: 0.515232\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.641 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7200/7500 (96%)] Loss: 0.378927\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.641 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.613490\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.641 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.641 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self bool\u001b[0m\n",
      "\n",
      "2021-03-01 16:10:26 Uploading - Uploading generated training model\u001b[34mINFO:__main__:Test set: Accuracy: 1.000000\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.641 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.644 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.644 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.644 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.644 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.644 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.648 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.648 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.648 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.648 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.648 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.657 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.665 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.673 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.676 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.676 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.679 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.683 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.683 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.683 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.683 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.683 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.687 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.687 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.687 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.687 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.687 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.688 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.691 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.691 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.691 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.691 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.691 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.695 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.695 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.695 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.695 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.695 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.696 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.698 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.699 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.699 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.699 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.699 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.703 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.703 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.703 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.703 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.703 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.704 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.706 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.706 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.706 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.706 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.706 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.710 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.710 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.711 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.711 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.711 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.711 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.714 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.714 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.714 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.714 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.714 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.718 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.718 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.718 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.718 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.718 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.719 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.722 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.722 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.722 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.722 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.722 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.722 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:21.724 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert BaseModelOutputWithPoolingAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:22.151 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:43:22.151 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:DataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/7500 (0%)] Loss: 1.072716\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [400/7500 (5%)] Loss: 0.807680\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [800/7500 (11%)] Loss: 0.872784\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1200/7500 (16%)] Loss: 0.660071\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1600/7500 (21%)] Loss: 0.202899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2000/7500 (27%)] Loss: 1.543220\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2400/7500 (32%)] Loss: 0.540362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2800/7500 (37%)] Loss: 0.628304\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/7500 (43%)] Loss: 0.630371\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3600/7500 (48%)] Loss: 0.678691\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.856 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.856 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.856 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.856 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.856 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.857 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.858 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.858 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.858 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m2021-03-01 16:10:24,192 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.858 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.858 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.859 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.859 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.859 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.859 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.859 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.860 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.861 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.861 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.861 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.861 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.861 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.862 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.862 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.862 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.862 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.862 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.863 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.864 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.864 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.864 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.864 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.864 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.865 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.865 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.866 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.866 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.866 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.866 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.867 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.867 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.867 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.867 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.867 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.869 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.869 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.869 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.869 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.869 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.869 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.870 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.870 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.870 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.870 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.870 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.872 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.872 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.872 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.872 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.872 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.872 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.873 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.873 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.874 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.874 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.874 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.875 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.875 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.875 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.875 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.875 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.875 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.877 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.877 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.877 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.877 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.877 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.878 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.878 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.878 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.878 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.879 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.879 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.880 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.880 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.880 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.880 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.880 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.881 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.881 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.882 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.882 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.882 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.882 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.883 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.883 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.883 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.883 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.883 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.884 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.885 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.885 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.885 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.885 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.885 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.886 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.886 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.886 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.886 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.886 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.887 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.888 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.888 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.888 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.888 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.888 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.889 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.889 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.889 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.889 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.889 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.891 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.891 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.891 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.891 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.891 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.891 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.892 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.892 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.892 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.893 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.893 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.893 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.893 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert BaseModelOutputWithPoolingAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.894 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-03-01 15:56:15.894 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:DataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4000/7500 (53%)] Loss: 0.718580\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4400/7500 (59%)] Loss: 0.970521\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4800/7500 (64%)] Loss: 0.720250\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5200/7500 (69%)] Loss: 0.097952\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5600/7500 (75%)] Loss: 1.449223\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6000/7500 (80%)] Loss: 0.764409\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/7500 (85%)] Loss: 0.752519\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6800/7500 (91%)] Loss: 0.515232\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7200/7500 (96%)] Loss: 0.378927\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.613490\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.649 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.650 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.651 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.651 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.651 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.651 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.652 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.653 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.653 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.653 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.653 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.653 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.654 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.654 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.655 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.655 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.655 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.655 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.656 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.657 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.657 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.657 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.657 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.657 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.658 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.659 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.660 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.661 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.661 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.661 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.661 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.661 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.662 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.663 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.663 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.663 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.663 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.663 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.664 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.665 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.665 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.665 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.665 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.665 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.666 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.667 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.667 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.667 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.667 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.667 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.668 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.669 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.669 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.670 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.670 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.670 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.670 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.671 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.671 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.671 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.671 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.671 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.671 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.672 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.673 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.674 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.674 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.674 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.674 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.674 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.675 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.676 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.677 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.677 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.677 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.677 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.677 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.678 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.678 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.678 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.678 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.678 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.679 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.679 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.680 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 bool\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.681 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder BaseModelOutputWithPastAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.682 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert BaseModelOutputWithPoolingAndCrossAttentions\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.682 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:08:08.682 algo-1:54 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:DataParallel SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 1.000000\n",
      "\u001b[0m\n",
      "\u001b[34mSaving tuned model.\u001b[0m\n",
      "\u001b[34m[2021-03-01 16:10:23.749 algo-1:54 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\n",
      "2021-03-01 16:11:34 Completed - Training job completed\n",
      "Training seconds: 1996\n",
      "Billable seconds: 1996\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# place to save model artifact\n",
    "output_path = \"s3://{}/{}\".format(bucket, prefix)\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\",\n",
    "    source_dir=\"train\",\n",
    "    role=role,\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p2.xlarge\",\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"num_labels\": 3,\n",
    "        \"batch_size\": 8,\n",
    "        \"test_batch_size\": 8\n",
    "    }\n",
    ")\n",
    "estimator.fit({\"training\": input_train, \"testing\": input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy our endpoint, we call deploy() on our PyTorch estimator object, passing in our desired number of instances and instance type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the predictor to use application/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.serializer = sagemaker.predictor._JsonSerializer\n",
    "predictor.deserializer = sagemaker.predictor._JsonDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yzoom piano roll a yzoom on the piano roll would be useful'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_frame['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-e180924442d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(\"predicted class: \", np.argmax(result, axis=1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36m_create_request_args\u001b[0;34m(self, data, initial_args, target_model, target_variant)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Body\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(training_data_frame['text'][0])\n",
    "#print(\"predicted class: \", np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the model for testing\n",
    "\n",
    "Once deployed, we can read in the test data and send it off to our deployed model to get some results. Once we collect all of the results we can determine how accurate our model is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
